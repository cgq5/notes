\title{Statistical learning notes}
\author{Guanqun Cao\\
\texttt{guanqun.cao@tut.fi}}
\date{\today}
\documentclass[11pt, a4paper]{article}
\usepackage{graphicx,amssymb,amstext,amsmath}
\usepackage{color}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{fullpage}
%\usepackage{makeidx}
%\makeindex
\newif\ifarial
%\arialtrue
\arialfalse
\ifarial
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}% use arial font
\else
\usepackage{pslatex} % Times New Roman
\fi
\hypersetup{colorlinks=true, linkcolor=blue!50!black}
\begin{document}
\graphicspath{{"/home/cao/Dropbox/thesis/Documents/Notes/statlearning/img/"}}
\maketitle
\pagestyle{plain}
\section{Introducton}
Before going into the data analysis, try to visualize it to have a first impression.\\
Difference between machine learning and statistical learning:
\begin{itemize}
\item Machine learning has a greater emphasis on \emph{large scale} applications and \emph{prediction accuracy}.
\item Statistical learning emphasizes \emph{models} and their interpretability, and \emph{precision} and \emph{uncertainty}.
\end{itemize}
The supervised learning problem: 
\begin{itemize}
  \item Outcome meansure $Y$ also called dependent variable, reponse, target.
  \item Vector of $p$ predictor measurements $X$ also called inputs, regressors, covariates, features, independent variables.
  \item In the \emph{regression problem}, $Y$ is quantitative (e.g. price, blood pressure).
  \item In the \emph{classification problem}, $Y$ takes values in a finite, unordered set.
  \item We have training data. These are observations of these measurements.
  \end{itemize}
  On the basis of the training data we would like to 
  \begin{itemize}
    \item Accurately predict unseen test cases.
    \item Understand which inputs affect the outcome and how
    \item Assess the quality of our predictions and inferences.
    \end{itemize}
To apply supervised learning, one has to understand the simple methods first, in order to grasp the more sophsiticated ones. It is important to accurately assess the performance of a method, to know how well or how badly it is working.\par
In unsupervised learning,
\begin{itemize}
  \item No output/dependent variable (response), just a set of predictors (features) measured on a set of samples.
  \item objective is more fuzzy --- find groups of samples that behave similarly, find features that behave similarly, find linear combinations of features with teh most variation.
  \item difficult to evaluate the method.
  \item can be used as a pre-processing step for supervised learning.
\end{itemize}
We denote the input vector as 
\[
  X = \begin{pmatrix} 
    X_1\\ X_2 \\ X_3 \end{pmatrix},
\]
where $X$ refers to the variable and $x$ denotes as the instance, and we usually take column vectors.\\
The ideal $f(x)=E(Y|X=x)$ is called the regression function. 
\begin{itemize}
  \item The regression function $\hat{f}(x)$ is the ideal or optimal predictor of $Y$ with regard to mean-squared predictor of $Y$ with regard to mean-squared prediction error: $f(x)=E(Y|X=x)$ is the function taht minimize $E[(Y-g(X))^2|X=x]$ over all functions $g$ at all points $X=x$.
  \item We also have $\epsilon=Y-f(x)$ as the irreducible error.
  \item For any estimate $\hat{f}(x)$ of $f(x)$, we have
\[
  E[(Y-\hat{f}(X))^2|X=x]=[f(x)-\hat{f}(x)]^2+\operatorname{Var}(\epsilon).
\]
\item We relax the definition and let
  \[
    \hat{f}(x)=\operatorname{Ave}(Y|X\in \mathrm{N}(x)),
  \]
  where $\mathrm{N}(x)$ is some \emph{neighborhood} of $x$.
\item Nearest neighbor average can be pretty good for small number of components $p$, i.e. $p\leq 4$ and large $N$.
\item Nearest neighbors tend to far away in high dimensions. (Curse of dimensionality). Note the curve about the curse of dimensionality. NN is to have the average estimate whose variance is $10\%$ lower than the original data. A $10\%$ neighborhood in high dimensions need no longer be local, so we cannot estimate $E(Y|X=x)$ by local averaging. 
\end{itemize}
Some trade-offs:
\begin{itemize}
\item Prediction accuracy versus interpretability
\item Good fit versus over-fit or under-fit
\item Parsimony versus black-box
\end{itemize}
\begin{figure}
[h!]
\centering
\includegraphics[width=.6\textwidth]{flex_vs_inter.png}
\caption{Interpretability vs flexibility.}
\end{figure}
We also have the Bias-Variance Trade-Off: The expected test MSE is
\[
  E(y_0-\hat{f}(x_0))^2=\operatorname{Var}(\hat{f}(x_0))+[\operatorname{Bias}(\hat{f}(x_0))]^2+\operatorname{Var}(\epsilon).
\]
\begin{itemize}
  \item The variance of a statistical learning method refers to the amount by which $\hat{f}$ would change if we estimated it using a different training dataset. In general, more flexible methods have higher variance.
  \item The Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
  \item As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. 
\end{itemize}
For classification, the Bayes optimal classifier at $x$ is 
\[
  C(x)=j \text{ if } p_j(x)=\max\{p_1(x),p_2(x),\dots,p_K(x)\},
\]
where the conditional class probabilities at $x$ is
\[
  p_k(x)=\operatorname{Pr}(Y=k|X=x), k=1,2,\dots,K.
\]
\begin{itemize}
  \item The performance of $\hat{C}(x)$ using the misclassification error rate is:
    \[
      {\operatorname{Err}}_{\text{\textbf{Te}}}=\underset{i\in \text{\textbf{Te}}}{\operatorname{Ave}}\;I[y_i\neq \hat{C}(x_i)].
    \]
\end{itemize}
For K-nearest neighbor, increasing K will reduce the training error continuously, but test error first drops and then rises.
\section{Linear Regression}
\textbf{Linear regression} is a simple approach to supervised learning. It assumes that the dependence of $Y$ on $X_1,X_2,\dots,Xp$ is linear.\\
We assume a model
\[
Y=\beta_0+\beta_1 X + \epsilon,
\]
where $\beta_0$ and $\beta_1$ are two unknown constants that represent the intercept and slope, also known as \emph{coefficients} or \emph{parameters}, and $\epsilon$ is the error term.
Given some estiamtes $\hat{\beta}_0$ and $\hat{\beta}_1$ for the model coefficients, we have the prediction
\[
  \hat{y}=\hat{\beta}_0+\hat{\beta}_1 x,
\]
where $\hat{y}$ indicates a prediction of $Y$ on the basis of $X=x$.\\
Estimation of the parameters by least squares
\begin{itemize}
\item We define $e_i=y_i-\hat{y}_i$ as the $i$th \emph{residual}, Then the \emph{rsidual sum of squares} (RSS) is defined as
\begin{align}
\operatorname(RSS)&=e^2_1+e^2_2+\dots+e^2_n,\\
&=(y_1-\hat{\beta}_0-\hat{\beta}_1x_1)^2+(y_2-\hat{\beta}_0-\hat{\beta}_1x_2)^2+\dots+(y_n-\hat{\beta}_0-\hat{\beta}_1x_n)^2.
\end{align}
\item The least squares approach chooses $\hat{\beta}_0$ and $\hat{\beta}_1$ to minimize the $\operatorname{RSS}$. The minimizing values are 
  \begin{align}
  \hat{\beta}_0&=\bar{y}-\hat{\beta}_1 \bar{x},\\
    \hat{\beta}_1&=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}.
  \end{align}
\end{itemize}
Assessing the accuracy of the coefficient estimates.\\
Suppose we have random sample $x_1,x_2,\dots,x_n$. The standard error of the mean (SEM) is the standard deviation of the sample-mean's estimate of a population mean.
\[
  \operatorname{SE}_{\bar{x}}=\frac{s}{\sqrt{n}}
\]
where $s$ is the sample standard deviation and $n$ is the number of observations of the sample.\\
This estimate can be cmopared with the formula for the true standard deviation of the sample mean as $T/n$ as $T=(x_1,x_2,\dots,x_n)$:
\[
  \operatorname{SD}_{\bar{x}}=\frac{\sigma}{\sqrt{n}}.
\]
The standard errors associated with $\hat{\beta}_0$ and $\hat{\beta}_1$ are
\[
  \operatorname{SE}(\hat{\beta}_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
\]
\[
  \operatorname{SE}(\hat{\beta}_0)^2 =\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}]
\]
\textbf{Confidence interval}:\\
A $95\%$ confidence interval is defined as a range of values such that $95\%$ probability, the range will contain the true unknown value of the parameter, i.e. $\hat{\beta}_1\pm 2\cdot \operatorname{SE}(\hat{\beta}_1)$. This is a frequentist idea.\\[1mm]
\textbf{Standard errors} can also be used to perform hypothesis tests on the coefficients. 
\begin{itemize}
\item To test the null hypothesis, we compute a \emph{t-statistic}, given by
\[
  t=\frac{\hat{\beta}_1-0}{\operatorname{SE}(\hat{\beta}_1)},
\]
It measures the number of standard deviations that $\beta_1$ is away from 0.
\item This will have a $t$-distribution with $n-2$ degrees of freedom, assuming $\beta_1=0$.
\item Using statistical tools, we can compute the probability of observing any value equal to $|t|$ or larger. We call this probability the \emph{p-value}.
\item We interpret the p-value as follows: with a small p-value 
we can infer there is an association between the predictor and the response.
\end{itemize}
\textbf{Degree of freedom:} 
\begin{itemize}
\item In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. It is the number of independent observations in a sample of data that are available to estimate a parameter of the population from which that sample is drawn. 
\item In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself, e.g.  the sample variance has N-1 degrees of freedom, since it is computed from N random scores minus the only 1 parameter estimated as intermediate step, which is the sample mean.
\end{itemize}
Assessing the overal accuracy of the model
\begin{itemize}
  \item We comptue the \emph{Residual Standard Error}
    \[
      \operatorname{RSE}=\sqrt{{1 \over n- 2} \operatorname{RSS}}=\sqrt{ {1 \over n-2} \sum_{i=1}^n (y_i-\hat{y}_i)^2},
    \]
    where the residual sum-of-squares is $\operatorname{RSS}=\sum_{i=1}^n (y_i-\hat{y}_i)^2$.
  \item \emph{R-squared} or fraction of variance explained is 
    \[
      R^2={\operatorname{TSS}-\operatorname{RSS} \over \operatorname{TSS} }=1-{\operatorname{RSS}\over\operatorname{TSS}}.
    \]
    where $\operatorname{TSS}=\sum_{i=1}^n (y_i-\bar{y}_i)^2$ is the total sum of squares.\\
$R^2$ is easier to interprete than RSE as it is between 0 and 1, but sometimes hard to determine what is a good $R^2$ value.
\item The $R^2$ statistic is the correlation between the two variables and measures how closely the input variable and the output variable are related. The $p$ value and $t$ statistic merely measure how strong is the evidence that there is a nonzero association. Even a weak effect can be extremely significant given enough data.\\[1mm] 
\end{itemize}
\textbf{Multiple Linear Regression}:\\[1mm]
It refers to regression models wth more than one predictor.
\begin{itemize}
\item Model: $Y=\beta_0+\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3$
\item we interpret $\beta_j$ as the \emph{average} effect on $Y$ of a one unit increase in $X_j$, holding all other predictors fixed.
\end{itemize}
Interpreting regression coefficients
\begin{itemize}
\item The ideal scenario is when the predictors are uncorrelated:
\begin{itemize}
\item Each coefficient can be estimated and tested separately
\item We can have the response depend on one predictors, with others fixed. 
\end{itemize}
\item The variance of all coefficients tends to increase, sometimes dramatically
\item Interpretations become hazadous
\item Claims of causality should be avoided for observational data.
\end{itemize}
Estimation and Prediction for Multiple Regression
\begin{itemize}
  \item Given estimates $\hat{\beta}_0, \hat{\beta}_1,\dots,\hat{\beta}_p$, we can make predictions using the formula 
\[
  \hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1 + \hat{\beta}_2x2+\cdots+\hat{\beta}_px_p
\]
\item The sum of squared residuals is
  \begin{align}
    \operatorname{RSS}&=\sum_{i=1}^n(y_i-\hat{y}_i)^2\\
    &=\sum_{i=1}^n(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_2x_{i2}-\cdots-\hat{\beta}_px_{ip})^2.
  \end{align}
  The values $\hat{\beta}_0,\hat{\beta}_1,\dots,\hat{\beta}_p$ minimize RSS are the multiple least squares regression coefficient estimates.
\end{itemize}
We define F-statistics
\[
  F=\frac{(\operatorname{TSS}-\operatorname{RSS}/p)}{\operatorname{RSS}/(n-p-1)}\sim F_{p,n-p-1}.
\]
\begin{itemize}
  \item If F-statistics is far larger than 1, it shows it is against the null hypothesis $H_0$.
  \item If F-statistic is significant, then we have extremely strong
    evidence that at least one of the predictors is associated with response.
\item If $n$ is large, an F-statistic that is just a little larger than 1 might still provide evidence against $H_0$, and vice versa.
\item When $H_0$ is true and the errors $\epsilon_i$ have a normal distribution, the F-statistic follows an F-distribution.
\item We can compute p-value from the F-statistic. And we can determine whether or not to reject $H_0$ based on the p-value.
\item There is a high chance to incorrectly conclude there is an association between the variable and the responce based on the individual t-statistics and associated p-values. The F-statistic does not suffer from this problem because it adjusts for the number of
  predictors.
\item If $p>n$ then there are more coefficients $\beta_j$ to estimate than observations from which to estimate them. We cannot fit the multiple linear regression model using least squares, so F-statistics cannot be used.
\end{itemize}
Deciding on the important variables
\begin{itemize}
  \item Forward selection: begin with the null model and add $p$ simple linear regressions that reults in the lowest RSS, until a stopping criterion is met.
  \item Backward selection: start with all variables in the model. Remove the one with the largest p-value iteratively, until a stopping criterion is met.
\end{itemize}
Qualitative Predictors
\begin{itemize}
\item They are discrete set of values.
\item Also called categorical predictors or factor variables.
\end{itemize}
Removing the additive assumption: \emph{interactions} and \emph{nonlinearity}.
\begin{itemize}
\item One input variable can influence another variable.
\item We can use the multiplication of the interacted variables to check the p-value of coefficent. It p-value is low, it indicates a strong interaction. 
\end{itemize}
Polynomial regression is the linear regression with non-linear regression functions.\\[1mm]
The hierarchy principle: If we include an interaction in a model, we should also include the main effects, even if the p-values assocated with their coefficients are not significant.\\
We can also have linear coefficients but non-linear functions of variable.\\[1mm]
Outliers\\[1mm]
Non-constant variance of error terms\\[1mm]
High leverage points\\[1mm]
Collinearity\\[1mm]
Statistical variability\\[1mm]
Scatter plot and box plot are two most common data visualization methods. For box plot
\begin{itemize}
\item The bottom and top of the box are always the first and third quartiles, and the band inside the box is always the second quartile (the median). 
\item The very bottom and top line are the hinges, which are the ranges.
\end{itemize}
\section{Classification}
We can use Linear Regression for binary classification problems, which is equivalent to \emph{linear discriminant analysis}. $E(Y|X=x)=\operatorname{Pr}(Y=1|X=x)$. But for multiclass problems, we cannot use linear regression.\\[1mm]
\subsection{Logistic Regression:}
We have $p(X)=\operatorname{Pr}(Y=1|X)$ and it has the form
\[
  p(X)=\frac{e^{\beta_0+\beta_1X}}{e^{1+\beta_0+\beta_1X}}
\]
$p(X)\in [0,1]$.\\
We also have the \emph{log odds} or \emph{logit transformation} of $p(X)$ as
\[
  \log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1 X.
\]
We use maximium likelihood to estimate the parameters.
\[
  \ell(\beta_0,\beta)=\prod_{i:y_i=1}p(x_i)\prod_{i:y_i=0}(1-p(x_i)).
\]
The likelihood gives the probabilitiy of the observed zeros and oens in the data. We pick $\beta_0$ and $\beta_1$ to maximize the likelihood of the observed data.\\[1mm]
Logistic regression with several variables
\[
  \log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X_1+\dots+\beta_pX_p
\]
\[
  p(X)=\frac{e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+\dots+\beta_pX_p}}.
\]
Confounding\\
Scatter plot: a type of mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are color-coded you can increase the number of displayed variables to three.\\[1mm]
Tilde in R:\\
The thing on the right of $\leftarrow$ is a formula object. It is often used to denote a statistical model, where the thing on the left of the $\sim$ is the response and the things on the right of the $\sim$ are the explanatory variables. So in English you'd say something like ``Species depends on Sepal Length, Sepal Width, Petal Length and Petal Width''.\\
Data Frame: The concept of a data frame comes from the world of statistical software used in empirical research; it generally refers to ``tabular'' data: a data structure representing cases (rows), each of which consists of a number of observations or measurements (columns). Alternatively, each row may be treated as a single observation of multiple ``variables''. In any case, each row and each column has the same data type, but the row (``record'') datatype may be heterogenous (a tuple of different types), while the column datatype must be homogenous. Data frames usually contain some metadata in addition to data; for example, column and row names.\\[1mm]
Case-control sampling and logistic regression
\begin{itemize}
  \item With case-control samples, we can estimate the regression parameters $\beta_j$ accurately if our model is correct, the constant term $\beta_0$ is incorrect.
  \item We can correct the estimated intercept by a simple transformation
  \[
    \hat{\beta}_0^*=\hat{\beta}_0+\log{\frac{\pi}{1-\pi}}-\log{\frac{\tilde{\pi}}{1-\tilde{\pi}}},
  \]
  where $\pi$ is the true probability and $\tilde{\pi}$ is the case probability.
  \item Often cases are rare and we take them all; up to five times that number of controls is sufficient.
  \item Case/Control sampling is most effective when the prior probabilities of the classes are very unequal. We expect this to be the case for the cancer and spam problems, but not the gender problem.
\end{itemize}
Logistic regression with more than two classes
\begin{itemize}
  \item One version used in R package \emph{glmnet} (Softmax function). It has the symmetric from
  \[
    \operatorname{Pr}(Y=k|X)=\frac{e^{\beta_{0k}+\beta_{1k}X_1+\dots+\beta_{pk}X_p}}{\sum_{\ell=1}^Ke^{\beta_{0\ell}+\beta_{1\ell}X_1+\dots+\beta_{p\ell}X_p}},
  \]
  where $K$ is the number of classes, and $K>2$. Each class has a linear function, and we weight against each other using an exponential function.
\item It is also referred to as multinomial regression.
\end{itemize}
\subsection{Discriminant Analysis}
\begin{itemize}
  \item It models the distribution of $X$ in each of the classes separately, and then use \emph{Bayes theorem} to flip things around and obtain $\operatorname{Pr}(Y|X)$.
  \item When we use normal distributions for each class, it leads to linear or quadratic discriminant analysis.
  \item It is quite generic, and applies to other distributions as well.
\end{itemize}
Bayes theorem for classification
From the Bayes theorem, 
\[
\operatorname{Pr}(Y=k|X=x)=\frac{\operatorname{Pr}(X=x|Y=k)\cdot\operatorname{Pr}(Y=k)}{\operatorname{Pr}(X=x)},
\]
Or
\[
P(Y \mid X) = \frac{P(X \mid Y)}{\sum_i {P(X \mid Y_i) P(Y_i)}} \cdot P(Y) 
\]
we can write the discriminant analysis as
\[
  \operatorname{Pr}(Y=k|X=x)=\frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)},
\]
where $f_k(x)=\operatorname{Pr}(X=x|Y=k)$ is the density for $X$ in class $k$, and we use normal densities separately for each class. $\pi_k=\operatorname{Pr}(Y=k)$ is the marginal or \emph{prior} probability for class $k$.\\[1mm]
Comparison between Logistic Regression and Discriminant Analysis
\begin{itemize}
  \item When the classes are well-separated, the parameter estimates for the logistic regression model are unstable. Linear Disciminant Analysis (LDA) does not suffer from this problem.
  \item If $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, LDA is more stable.
  \item LDA is popular when we have more than two classes, because is also provides low-dimensional views of the data.
\end{itemize}
Linear Discriminant Analysis when $p=1$\\
\[
f_k(x) \sim \mathcal{N}(\mu_k,\,\sigma_k^2),
\]
where $\mu_k$ and $\sigma_k^2$ are the mean and variance of class $k$ respectively, and we assume all $\sigma_k=\sigma$ are the same.\\
We can plug in the former equation into the bayes formula. and perform some cancellation and simplification. To classify $X=x$, we need to find the largest $p_k(x)$. Taking logs, and discarding terms that do not depend on $k$, we see that it is equivalent to assigning $x$ to the class with the largest \emph{discriminant score}:
\[
  \delta_k(x)=x\cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{\sigma^2}+\log(\pi_k).
\]
$\delta_k(x)$ is a \textbf{linear} function of $x$.
Parameter estimation
\begin{align}
  \hat{\pi}_k&=\frac{n_k}{n}\\
  \hat{\mu}_k&= \frac{1}{n_k} \sum_{i:y_i=k}x_k\\
  \hat{\sigma}^2&=\frac{1}{n-K} \sum_{k=1}^K \sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2\\
 &=\sum_{k=1}^K\frac{n_k-1}{n-K}\cdot \hat{\sigma}_k^2.
\end{align}
where $\hat{\sigma}^2_k=\frac{1}{n_k-1}\sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2$ is the usual formula for the estimated variance in the $k$th class.\\[1mm]
Linear Discriminant Analysis when $p>1$\\
Density: 
\[f(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-{1\over 2}(x-\mu)^\top\Sigma^{-1}(x-\mu)}.
\]
Discriminant function:
\[
  \delta_k(x)=x^\top\Sigma^{-1}\mu_k-{1\over 2}\mu_k^T\Sigma^{-1}\mu_k+\log\pi_k.
\]
but it remains linear.\\
Fisher's Disciminant Plot:\\
When there are $K$ classes, linear discriminant analysis can be viewed in a $K-1$ dimensional plot. It classifies to the closest centroid, and they span a $K-1$ dimensional plane.\\[1mm]
Probabilisitic interpretation\\
Once we have estimates $\hat{\delta_k}(x)$, we can turn these functions into estimates for class probabilities:
\[
 \hat{\operatorname{Pr}}(Y=k|X=x)=\frac{e^{\hat{\delta}_k(x)}}{\sum_{l=1}^Ke^{\hat{\delta}_l(x)}}.
\]
The ROC curve shows true positive rate and false positive rate simultaneously. The curve traces out when changing the threshold. We also use the AUC or area under the curve to compare the performance between classifiers with different thresholds. Higher AUC is good.\\[1mm]
Other forms of Discriminant Analysis\\
\[
  \operatorname{Pr}(Y=k|X=x)=\frac{\pi_kf_k(x)}{\sum^K_{l=1}\pi_lf_l(x)}
\]
When $f_k(x)$ are Gaussian densities, with the same covariance matrix $\Sigma$ in each class, it leads to LDA. By altering the forms for $f_k(x)$, we get different classifiers as
\begin{itemize}
  \item With Gaussians but different $\Sigma_k$ in each class, we get quadratic discriminant analysis.
  \item With $f_k(x)=\prod^p_{j=1}f_{jk}(x_j)$ (conditional independence model) in each class we get naive Bayes. For Gaussian this means the $\Sigma_k$ are diagonal.
  \item Many other forms, by proposing specific density models for $f_k(x)$, including nonparametric approaches.
  \end{itemize}
  Quadratic Discriminant Analysis
  \[
    \delta_k(x)=-{1\over2}(x-\mu_k)^\top \Sigma_k^{-1}(x-\mu_k)+\log\pi_k-{1\over2}\log|\Sigma_k|
  \]
  The quandratic terms matter as $\Sigma_k$ are different.\\[1mm]
Naive Bayes\\
Assume features are independent in each class. NB is Useful when $p$ is large, and multiviariate methods like QDA and even LDA break down due to large covariance matrices.
\begin{itemize}
  \item Gaussian naive Baye assumes each $\Sigma_k$ is diagonal:
    \begin{align}
    \delta_k(x) & \sim \log\bigg[\pi_k\prod_{j=1}^pf_{ij}(x_j)\bigg]\\
    & = -{1\over2}\sum_{j=1}^p\bigg[\frac{(x_j-\mu_{kj})^2}{\sigma^2_{kj}}+\log\sigma^2_{kj}\bigg]+\log\pi_k.
  \end{align}
\item can use for mixed feature vectors (qualitative and quantitative). If $X_j$ is qualitative, replace $f_{kj}(X_j)$ with probability mass function (histogram) over discrete categories.
\end{itemize}
Compare Logistic Regression and LDA\\
For a two-class problem, one can show for LDA
\[
  \log\bigg({p_1(x)\over{1-p_1(x)}}\bigg)=\log\bigg(\frac{p_1(x)}{p_2(x)}\bigg)=c_0+c_1x_1+\dots+c_px_p.
\]
It has the same form as logistic regression. The difference is in how the parameters are estimated.
\begin{itemize}
  \item Logistic regression uses the conditional likelihood based on $\operatorname{Pr}(Y|X)$, known as discriminative learning.
  \item LDA uses the full likelihood based on $\operatorname{Pr}(X,Y)$, known as generative learning.
  \item Despite these differences, in practice the results are often very similar.
\end{itemize}
\section{Resampling}
Validation method: We split the original training set into a train and validation set. Then, we fit models of various set sizes and of various model sizes. Our job is to find $\hat{k}$, and return the model $\mathcal{M}_{\hat{k}}$.
\begin{itemize}
  \item two major methods: cross validation and the bootstrap
  \item Resampling refits a model of interest to samples formed from the training set, in order to obtain addtional information about the fitted model.
  \item They provide estimates of test set prediction error, and the standard deviation and bias of our parameter estimates.
  \end{itemize}
Drawbacks of validation set approach
\begin{itemize}
\item the validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set
\item Only a subset of the observation are used for training
\item The validation set error may tend to overestimate the test error for the model fit on the entire dataset. 
\end{itemize}
K-fold cross validation
\begin{itemize}
\item random divides the data into $K$ equal-size parts. We leave out. We leave out part $k$, fit the model to the other $K-1$ parts (combined), and then obtain predictions for the left-out $k$th part.
\item Let the $K$ parts be $C_1,C_2,\dots C_K$ where $C_k$ denotes the indices of the observations in part $k$. There are $n_k$ observations in part $k$: if $N$ is a multiple of $K$, then $n_k=n/K$.
\item Computer
  \[
    CV_{(K)}=\sum_{k=1}^K{n_k\over n}\operatorname{MSE}_k,
  \]
  where $\operatorname{MSE}_k\sum_{i\in C_k}(y_i-\hat{y}_i)^2/n_k$ and $\hat{y}_i$ is the fit for observation $i$, obtained from the data with part $k$ removed.
\item Setting $K=n$ yield $n$-fold or leave-one out cross validation.
\end{itemize}
Leave-one-out cross validation
\begin{itemize}
  \item With least squares linear or polynomial regression, an amazing shortcut maeks the cost of leave-one out cross validation the same as that of a single model fit. 
\[
  \operatorname{CV}_{(n)}={1\over n}\sum_{i=1}^n\bigg(\frac{y_i-\hat{y}_i}{1-h_i}\bigg)^2.
\]
where $\hat{y}_i$ is the $i$the fitted value from the original least squares fit, and $h_i$ is the leverage. It is like the ordinary MSR, except the $i$the residual is divided by $1-h_i$.
\item It does not shake up the data enough. The estimates from each fold are highly correlated and hence their average can have high variance.
\item $K=5, 10$ is a better choice.
\end{itemize}
  Problems with Cross validation
\begin{itemize}
\item Since each training set is only $(K-1)/K$ as big as the original training set, the estimates of prediction error will typically be biased upward.
\item The biase is minimized when $K=n$ (leave one out), the estimae has high variance.
\item $K=5 \text{or} 10$ provides a better compromise.
\end{itemize}
Cross Validation for Classification
\begin{itemize}
\item Compute
  \[
    CV_K=\sum_{k=1}^K{n_k\over n}\operatorname{Err}_k
  \]
  where $\operatorname{Err}_k=\sum_{i\in C_k}I(y_i\neq \hat{y}_i)/n_k$.
\item The estimated standard deviation of $\operatorname{CV}_k$ is 
\[
  \hat{\operatorname{SE}}(CV_k)=\sqrt{\sum_{k=1}^K\operatorname{Err}_k-\bar{\operatorname{Err}_k}^2/(K-1)}.
\]
\end{itemize}
Cross validation: right and wrong
\begin{itemize}
\item If we pre-select the predictors correlated to the class labels, we should not ignore this fact when applying cross validation. The procedure has already seen the labels of the training data and made use of them. It is a form of training and should be included in the validation process.
\item The right way is to apply screening and validation at each round of cross validation.
\end{itemize}
The bootstrap
\begin{itemize}
  \item For real data, we cannot generate new samples from the original population.
  \item Bootstrap allows to use computer to mimic the processing of obtaining new data sets, so that we can estimate the variability of our estimate without generating additonal samples.
  \item Rather than repeatedly obtaining independent datasets from the population, we obtain distinct datasets by repeatedly sampling obserations from the original dataset with \emph{replacement}.
  \item The procedure is repeated $B$ times for some large value of $B$, in order to produce $B$ different bootstrap datasets, $Z^{*1},Z^{*2},\dots Z^{*B}$ and $B$ corresponding $\alpha$ estimates, $\hat{\alpha}^{*1},\hat{\alpha}^{*2}\dots \hat{\alpha}^{*B}$.
  \item We estimate the standard error of these bootstrap estimates using the formula
\[
  \operatorname{SE}_B(\hat{\alpha})=\sqrt{{1\over {B-1}}\sum_{r=1}^B(\hat{\alpha}^{*r}-\bar{\hat{\alpha}}^*)^2}
\]
\end{itemize}
\begin{figure}
[h!]
\centering
\includegraphics[width=.8\textwidth]{bootstrap.png}
\caption{Sampling and resampling.}
\end{figure}
\begin{itemize}
  \item In time series data, where data is not iid., we cannot bootstrap the data with replacement, but we can apply block boostrap.
  \item Primarily used to obtain standard erros of an estimate
\item also provides approximate confidence intervals for a population parameter, which represents an approximate $90\%$ confidence intervial for the true $\alpha$.
\item The above interval is called a \emph{Bootstrap Percentile} confidence interval. It is the simplest method for obtain a confidence interval from the bootstrap.
\end{itemize}
Compare Bootstrap and Cross Validation
\begin{itemize}
\item In CV, there is no overlap
\item To estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original samples as our validation set.
\item But each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. 
\end{itemize}
\section{Linear Model Selection and Regularization}
\subsection{Subset selection}
The reason to alternate Least Square methods
\begin{itemize}
\item Prediction Accuracy: when $p>n,$, to control the variance.
\item Model Interpretability: By setting the corresponding coefficient estimates to zero --- we can obtain a model that is more easily interpreted. Feature selection.
\end{itemize}
Three major classes of methods
\begin{itemize}
  \item \emph{Subset Selection}: We identify a subset of the $p$ predictors that we believe to be related to the response. We then fit the model using least squares on the reduced set of variables.
  \item \emph{Shrinkage:}: We fit a model involving all $p$ preditors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. The shrinkage aka regularization has the effect of reducing variance and can perform variable selection.
  \item \emph{Dimension Reduction}. We project the $p$ predictors into a $M$-dimensinoal subspace, where $M<p$. This is achieved by computing $M$ different linear combinations, or projections of the variables. Then these $M$ projections are used predictors to fit a linear regression model by least square. 
\end{itemize}
Best Subset Selection
\begin{enumerate}
\item Let $M_0$ denote the \emph{null model}, which contain no predictors. This model simply predicts the sample mean for each observation.
\item For $K=1,2,\dots p$:
\begin{enumerate}
\item Fit all $p \choose k$ models that contain exactly $k$ predictors.
\item Pick the best among these $p \choose k$ models, and call it $\mathcal{M}_k$. Choose the best model having the smallest RSS, or largest $R^2$.
\end{enumerate}
\item Select a single best frmo among $M_0,M_1,\dots,M_p$ using cross-validated prediction error, $C_p$ (AIC), (BIC), or adjusted $R^2$.
\end{enumerate}
Subset Selection applies to many methods, including least square regression, logistic regression, and so on. The deviance--- negative two times the maximized log-likelihood---plays the role of RSS for a broader class of models.\\[1mm]
Stepwise Selection
\begin{itemize}
\item Best subset selection does not apply with very large $p$. It leads to a large search space, and overfitting. And the complexity is $O(2^p)$.
\item Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.
\item at each step the variable that gives the greatest additional improvement to the fit is added to the model.
\item Complexity: $O(p^2)$
\end{itemize}
The other around is Backward Stepwise Selection.
\begin{itemize}
  \item It searches through $1+p(p+1)/2$ models, same as forward subset selection.
  \item Backward selection requires the number of sample $n$ larger than the number of variables $p$, so that the full model can be fit. But forward does not have the requirement.
  \end{itemize}
  \subsection{Some criterions}
$C_p$, AIC, BIC and adjusted $R^2$
\begin{itemize}
  \item They adjust the training error for the model size, and can be used to select among a set of models with different number of variables.
  \item Mallow's $C_p$
    \[
      C_p={1\over n}(\operatorname{RSS}+2d\hat{\sigma}^2),
    \]
    where $d$ is the total number of parameters used and $\hat{sigma}^2$ is an estimate of the variance of the error $\epsilon$ associated with each response measurement. And $n>p$.
\item The AIC criterion is defined for a large class of models fit by maximum likelihood:
\[
\operatorname{AIC} = -2\log L+2\cdot d,
\]
where $L$ is the maximized value of the likelihood function for the estimated model.
\item In the case of the linear model with Gaussian errors, maximium likelihood and least squares are the equivalent. 
  \[
    -2\log L =\operatorname{RSS}/\hat{\sigma}^2
  \]
  \end{itemize}
  Bayesian Information Criterion (BIC)
\begin{itemize}
  \item $\operatorname{BIC}={1\over n}(\operatorname{RSS}+\log(n)d\hat{\sigma}^2).$
  \item Like $C_p$, the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.
  \item BIC replaces the $2d\hat{\sigma}^2$ used by $C_p$ with a $\log(n)d\hat{\sigma}^2$ term, where $n$ is the number of observations.
  \item Since $\log n > 2$ for any $n>7$, the BIC statistics generally places a heavier penalty on models with many variables, and hence results in the slection of smaller models than $C_p$.
\end{itemize}
Adjusted $R^2$
\begin{itemize}
\item For a least squares model with $d$ variables, the adjusted $R^2$ statistic is calculated as 
    \[
      \text{Adjusted }R^2=1-\frac{\operatorname{RSS}/(n-d-1)}{\operatorname{TSS}/(n-1)}.
    \]
    where TSS is the total sum of squares.
\item Unlike $C_p$, AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted $R^2$ indicateds a model with a small test error.
\item Maximizing the adjusted $R^2$ is equivalent to minimizing $\operatorname{RSS}/(n-d-1).$ While RSS always decreases as the number of variables in the model inceases, $\operatorname{RSS}/(n-d-1)$ may increase or decrease, due to the presence of $d$ in the denominator.
\item unlike the $R^2$ statistic, the adjusted $R^2$ statstic pays a price for the inclusion of unnecessary variables in the model.
\end{itemize}
Validation and Cross-Validation
\begin{itemize}
  \item The procedure has an adavantage over AIC, BIC, $C_p$ and adjusted $R^2$, in that it provides a direct estimate of the test error, and does not require an estimate of the error variance $\sigma^2$. 
  \item It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (the number of predictors in the model) or hard to estimate the error variance $\sigma^2$.
\end{itemize}
One-standard-error rule: We first calculate the standard error of the estimated test MSE for each model size, and the select the smallest model for which the estimated test error is within oen standard error of the lowest point on the curve.\\[1mm]
\subsection{Shrinkage Methods}
Ridge regression and Lasso
\begin{itemize}
\item The subset selection methods use least squares to fit a linear model that contains a subset of predictors.
\item Alternatively, we can fit a model containing all $p$ predictors using a technique that constrains and regularizes the coefficient estimates, or shrinks the coefficient estimates towards zero.
\item It may not be immediately obvious why such as contraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.
\end{itemize}
Ridge Regression
\[
  \sum_{i=1}^n\bigg(y_i-\beta_0-\sum_{j=1}^p\beta_j x_{ij}\bigg)^2+\lambda\sum_{j=1}^p\beta_j^2=\operatorname{RSS}+\lambda\sum_{j=1}^p\beta_j^2,
\]
where $\lambda\geq 0$ is a tuning parameter, to be determined separately.\\[1mm]
\begin{itemize}
\item As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.
\item However, the second term $\lambda\sum_j\beta_j^2$ called a shrinkage penalty, is small when $\beta_1,\dots,\beta_p$ are close to zero, and so it has the effect of shrinking the estimates of $\beta_j$ towards zero.
\item The tuning parameter $\lambda$ serves to control the relative impact of these two terms on the rgerssion coefficient estimates.
\item Selecting a good value for $\lambda$ is ciritical, cross validation is used for this.
  \end{itemize}
  Ridge Regression: scaling of predictors
\begin{itemize}
  \item The standard least squares coefficient estimates are scale equivariant: multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the $j$th predictor is scaled, $X_j\hat{\beta}_j$ will remain the same.
  \item In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.
  \item It is best to apply ridge regression after standardizing the predictors, using the formula
\[
\tilde{x}_{ij}=\frac{x_{ij}}{{1\over n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}
\]
\end{itemize}
One disadvantage of Ridge Regression is instead of involving a subset of variables in subset selection, ridge regression will include all $p$ predictors in the final model.\\
The Lasso is a relatively new alternative to ridge regression that overcomes the disadvantage. The lasso coefficients, $\hat{\beta}_\lambda^L$ minimize the quantity
\[
  \sum_{i=1}^n\bigg(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\bigg)+\lambda\sum_{j=1}^p|\beta_j|=\operatorname{RSS}+\lambda\sum_{j=1}^p|\beta_j|.
\]
In statistical parlance, the lasso uses an $\ell_1$ penalty instead of an $\ell_2$ penalty. The $\ell_1$ norm of a coefficient vector $\beta$ is given by $\|\beta\|_1=\sum|\beta_j|$.
\begin{itemize}
\item As with ridge regression, the lasso shrinks the coefficients estimates towards zero.
\item In the case of lasso, the $\ell_1$ penalty ahs the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning paramter $\lambda$ is suffciently large.
\item Much like best subset selection, the lass performs variable selection.
\item We say that the lasso yields sparse models --- that is models that involve only a subset of the variables.
\item As in ridge regression, selecting a good value of $\lambda$ for the lasso is critical; cross-validation is the method of choice.
\end{itemize}
\begin{figure}
[h!]
\centering
\includegraphics[width=.8\textwidth]{lasso.png}
\caption{Lasso shrinks several variables towards zero.}
\end{figure}
We can see from the graph that by increasing $\lambda$, we can shrink some variables to exactly zero.\\
The Variable Selection Property of the Lasso\\
Why is it that the lasso, unlike ridge regressio, results in coefficent estimates that are exactly equal to zero? It is equivalent to say the lasso and ridge regression coefficient estimates solve the problems
\begin{align}
  \min_\beta&\sum_{i=1}^n\bigg(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\bigg)^2\\
  \text{ subject to }& \sum_{j=1}^p|\beta_j|\leq s.
\end{align}
and 
\begin{align}
  \min_\beta&\sum_{i=1}^n\bigg(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\bigg)^2\\
  \text{ subject to }& \sum_{j=1}^p\beta_j^2\leq s.
\end{align}
\begin{figure}
[h!]
\centering
\includegraphics[width=.6\textwidth]{lasso_solution.png}
\caption{The lasso solution reaches the corner, which gives the sparsity.}
\end{figure}\\
Selecting the tuning parameter for ridge regression and lasso using cross validation\\
We choose a grid of $\lambda$ values, and compute the cross validation error rate for each value of $\lambda$. We then select the tuning parameter avlue for which the cross validation error is the smallest. $d\leq p$ but is unknown, so BIC, AIC and adjusted $R^2$ cannot be used.
\subsection{Dimension Reduction Methods}
\begin{itemize}
  \item Let $Z_1,Z_2,\dots,Z_M$ represent $M<p$ linear combinations of our original $p$ predictors. That is
\[
  Z_m=\sum_{j=1}^p\phi_{mj}X_j
\]
for some constants $\phi_{m1},\dots,\phi_{mp}$.
\item We can then fit the linear regression model
  \[
    y_i=\theta_0+\sum_{m=1}^M\theta_m z_{im}+\epsilon_i, i=1,\dots,n
  \]
using ordinary least squares.
\item Note that in the second mode, the regression coefficients are given by $\theta_0,\theta_1,\dots,\theta_M$. If the constants $\phi_{m1},\dots,\phi_{mp}$ are chosen wisely, the dimension reduction approaches can often outperform OLS regression.
\item We do the transform as
  \[
    \sum_{m=1}^M\theta_m z_{im}=\sum_{m=1}^M\theta_m\sum_{j=1}^p \phi_{mj}x_{ij}=\sum_{j=1}^p\sum_{m=1}^M\theta_m\phi_{mj}x_{ij}=\sum_{j=1}^p\beta_jx_{ij}
  \]
  where
  \[
    \beta_j=\sum_{m=1}^M\theta_m\phi_{mj}.
  \]
\item Dimension reduction serves to constrain the estimated coefficients, since now th ey must take the form as above.
\end{itemize}
Principal Component Regression
\begin{itemize}
  \item PCR identifies linear combations or direction that best represent the predictors $X_1,\dots,X_p$. 
  \item These directions are identifies in an unsupervised way, since the response $Y$ is not used to help determine th e principal component directions.
  \item The response does not supervise the identification of the principal component. 
  \item There is no guarantee that the directions represent the predictors are the bes t directions for predicting the response.
\end{itemize}
Partial Least Squares (PLS)
\begin{itemize}
\item Unlike PCR, PLS identifies the new features in a supervised manner --- it makes use of the response $Y$ in order to identify new featuers that not only approximate the old features well, but are related to the response.
\item Rougthly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.
\end{itemize}
PLS
\begin{itemize}
  \item After standardizing the $p$ predictors, PLS computes the first direction $Z$ by setting each $\phi_{1j}$ equal to the coefficient from the simple linear regression of $Y$ onto $X_j$.
  \item One can show this coefficient is proportional to the correlation between $Y$ and $X_j$.
  \item In computing $Z_1=\sum^p_{j=1}\phi_{1j}X_j$, PLS places the highest weight on the variables that are most strongly related to the response.
  \item Subsequent directions are found by taking residuals and then repeating the above prescription.
\end{itemize}
Polynomial Regression
\begin{itemize}
  \item \[
y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\dots+\beta_3x_i^d+\epsilon_i
\]
\item We look into the fitted function values at any value $x_0$:
  \[
    \hat{f}(x_0)=\hat{\beta}_0+\hat{\beta}_1x_0+\hat{\beta}_2x_0^2+\dots+\hat{\beta}_kx_0^k
  \]
\item Since $\hat{f}(x_0)$ is a linear function of the $\hat{\beta}_\ell$, we can get simple expression for pointwise-variances $\operatorname{Var}[\hat{f}(x_0)]$ at any value $x_0$.
\item We either fix the degree $d$ at some reasonble low value, or use cross validation.
\item Nonlinear Logistic Regression
  \[
    P(Y>250|X=x_i) = \frac{\exp(\beta_0+\beta_1x_i+\beta_2x_i^2+\dots+\beta_dx_i^d)}{1+\exp(\beta_0+\beta_1x_i+\beta_2x_i^2+\dots+\beta_dx_i^d)}
  \]
\item To get confidence intervals, compute upper and lower bounds \emph{on the logit scale}, and then invert to get on probabiility scale.
\item Can do separately on several variables --- just stack the variables into one matrix, and separate out the pieces afterwards
\item Caveat: polynomials have notrious tail behavior -- very bad for extrapolation.
\end{itemize}
Step Functions\\
Another way of creating transformations of a variable --- cut the variable into distinct regions. It is a local model. It is easy to work with, which creates a series of dummy variables representing each group.\\[1mm]
Piecewise Polynomials
\begin{itemize}
\item Instead of a single polynomial in $X$ over its whole domain, we can rather use different polynomials in regions defined by knots.
  \[
    y_i= \begin{cases} 
      \beta_{01}+\beta_{11}x_i+\beta_{21}x_i^2+\beta_{31}x_i^3+\epsilon_i \text{ if }x_i<c; \\
      \beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x_i^3+\epsilon_i \text{ if }x_i<c; \\
    \end{cases} 
  \]
\item Better to add constraints to the polynomials, e.g. continuity
\item Splines have the ``maximum'' amount of continuity
\end{itemize}
\subsection{Nonline methods}
Linear Splines\\
A linear spline with knots at $\epsilon_k, k=1,\dots,K$ is a piecewise linear polynomial continuous at each knot.\\
We can represent this model as
\[
  y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+\dots+\beta_{K+3}b_{K+3}(x_i)+\epsilon_i,
\]
where the $b_k$ are basis functions.
\begin{align}
  b_1(x_i)&=x_i\\
  b_{k+1}(x_i)&=(x_i-\xi_k)_+, \text{    }k=1,\dots,K
  \end{align}
  Here the $()_+$ means positive part, i.e.
  \[
    (x_i-\xi_k)_+=
    \begin{cases}x_i-\xi_k \text{ if }x_i>\xi_k;\\
      0\text{  otherwise}
    \end{cases}
  \]
  Cubic Splines
  A cubic spline with knots at $\xi_k$, $k=1,\dots,K$ is piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.
  We describe the model with truncated power basis functions
  \[
    y=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+\dots+\beta_{K+1}b_{K+1}(x_i)+\epsilon_i,
  \]
\begin{align}
  b_1(x_i)&=x_i\\
  b_2(x_i)&=x_i^2\\
  b_3(x_i)&=x_i^3\\
  b_{k+3}(x_i)&=(x_i-\xi_k)^3_+, \text{    }k=1,\dots,K
  \end{align}
  \[
    (x_i-\xi_k)^3_+=
    \begin{cases}(x_i-\xi_k)^3 \text{ if }x_i>\xi_k;\\
      0\text{  otherwise}
    \end{cases}
  \]
Natural Cubic Splines\\[1mm]
A natural cubic spline extraplates linearly beyond the boundary knots. This adds 4 extra contraints, and allow us to put more internal knots for the smae degre of freedom as a regular cubic spline.
\begin{figure}
[h!]
\centering
\includegraphics[width=.5\textwidth]{cubic_splines.png}
\caption{It shows the difference between Natural Cubic Spline and Cubic Spline.}
\end{figure}
Knot placement
\begin{itemize}
\item One strategy is to decide $K$, the number of knots, and then palce them at appropriate quantiles of the observed $X$.
\item A cubic spline with $K$ knots has $K+4$ paramters or degree of freedom.
\item A natural spline with $K$ knots has $K$ degrees of freedom.
\end{itemize}
Smoothing Splines\\
Consider fitting a smooth function to some data
\[
  \min_{g\in \mathcal{S}}=\sum_{i=1}^n(y_i-g(x_i))^2+\lambda\int g''(t)^2dt
\]
\begin{itemize}
\item The first term is RSS, and tries to make $g(x)$ match data at each $x_i$.
\item The second term is a \emph{roughness penalty} and controls how wiggly $g(x)$ is. It is modulated by the tuning parameter $\lambda\geq 0$.
  \begin{itemize}
    \item The smaller $\lambda$, the more wiggly the function, eventually interpolating $y_i$ when $\lambda=0$.
    \item As $\lambda \rightarrow \infty$, the function $g(x)$ becomes linear.
    \item The solution is a natural cubic spline, with a knot at every unique value of $x_i$. The roughness peanlty still controls the roughness via $\lambda$.
  \end{itemize}
\end{itemize}
Some details about Smoothing Spline
\begin{itemize}
  \item Smoothing splines avoid the knot-selection issue, leaving single $\lambda$ to be chosen.
  \item The vector of $n$ fitted values can be written as $\hat{g}_\lambda=\mathbf{S}_\lambda \mathbf{y}$, where $\mathbf{S}_\lambda$ is a $n\times n$ matrix (determined by the $x_i$ and $\lambda$)
\item The effective degrees of feedom are given by 
\[
  df_\lambda=\sum_{i=1}^n\{\mathbf{S}_\lambda\}_{ii}.
\]
\item We can specify $df$ rather than $\lambda$.
\end{itemize}
Local Regression\\
With a sliding weight function, we fit separate linear fits over the range of $X$ by weighed least squares.\\[1mm]
Generalized Additive Models\\
Allows for flexible non-linearities in several variables, but retain the additive structure of linear models.
\[
  y_i=\beta_0+f_1(x_{ij})+f_2(x_{i2})+\dots+f_p(x_{ip}) + \epsilon_i
\]
GAMs for classification
\[
  \log(\frac{p(X){1-p(X)}})=\beta_0+f_1(X_1)+f_2(X_2)+\dots+f_p(X_p).
\]
\section{Tree-based methods}
\begin{itemize}
\item It involves stratifying or segmenting the predictor space into a number of simple regions.
\item Since the set of splitting rules used to segment the predictor psace can be summarized in a tree, these types of approaches are known as decision-tree methods.
\end{itemize}
Terminology for Trees
\bibliographystyle{template}
\bibliography{/home/cao/Dropbox/ResearchBib.bib}
\end{document}
